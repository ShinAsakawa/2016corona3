{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4回Pythonで実践する深層学習勉強会\n",
    "\n",
    "<center>\n",
    "浅川伸一 __asakawa@ieee.org__\n",
    "</center>\n",
    "\n",
    "# 固有顔とフィッシャー顔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文献\n",
    "1. Turk and Pentland (1991)\n",
    "2. Belhumeur, Hespanha and Kriegman (1997)\n",
    "\n",
    "参考\n",
    "[Linear Discriminant Analysis -Bit by Bit](http://sebastianraschka.com/Articles/2014_python_lda.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 固有顔 [PCA 主成分分析](https://en.wikipedia.org/wiki/Principal_component_analysis)ベース\n",
    "- フィッシャー顔 [LDA 線形判別分析](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)ベース\n",
    "\n",
    "この場合 LDA: <font color=\"blue\">Linear Discriminant Analysis</font> であって <font color=\"blue\">Latent Dirichlet Allocation</font> ではないことに注意\n",
    "\n",
    "統計学の知識のある方であれば，同様の考え方をすれば ICA 顔，NMF 顔，なども考えられるし実際に提案されている。ただし，今回は触れない。\n",
    "\n",
    "[scikit-learn](face_recognition.ipynb)の固有顔のデモは LFW 日常的な画像に人物ラベルを付けたデータセットを用いてPCAを実行し，識別に SVM を用いるものであった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import six\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "#from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:\n",
      "n_samples: 1288\n",
      "n_features: 1850\n",
      "n_classes: 7\n"
     ]
    }
   ],
   "source": [
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2 data directly (as relative pixel\n",
    "# positions info is ignored by this model)\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split into a training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 150 eigenfaces from 1159 faces\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n"
     ]
    }
   ],
   "source": [
    "num_features = n_components = 150\n",
    "\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
    "      % (n_components, X_train.shape[0]))\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized',\n",
    "          whiten=True).fit(X_train)\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.variables.Variable object at 0x11b482b50>\n",
      "<tensorflow.python.ops.variables.Variable object at 0x11b477d90>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# resets the graph, needed when initializing weights multiple times, like in this notebook\n",
    "reset_default_graph()\n",
    "\n",
    "# Setting up placeholder, this is where your data enters the graph!\n",
    "x_pl = tf.placeholder(tf.float32, [None, num_features])\n",
    "\n",
    "# defining our initializer for our weigths from a normal distribution (mean=0, std=0.1)\n",
    "weight_initializer = tf.truncated_normal_initializer(stddev=0.1)\n",
    "with tf.variable_scope('l_1'): # if you run it more than once, reuse has to be True\n",
    "    W_1 = tf.get_variable('W', [num_features, num_output], # change num_output to 100 for mlp\n",
    "                          initializer=weight_initializer)\n",
    "    b_1 = tf.get_variable('b', [num_output], # change num_output to 100 for mlp\n",
    "                          initializer=tf.constant_initializer(0.0))\n",
    "with tf. variable_scope('l_2'):\n",
    "    W_2 = tf.get_variable('W', [100, num_output],\n",
    "                          initializer=weight_initializer)\n",
    "    b_2 = tf.get_variable('b', [num_output],\n",
    "                          initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "# Setting up ops, these ops will define edges along our computational graph\n",
    "# The below ops will compute a logistic regression, but can be modified to \n",
    "# compute a neural network\n",
    "\n",
    "l_1 = tf.matmul(x_pl, W_1) + b_1\n",
    "# to make a hidden layer we need a nonlinearity\n",
    "# l_1_nonlinear = tf.nn.relu(l_1)\n",
    "# the layer before the softmax should not have a nonlinearity\n",
    "# l_2 = tf.matmul(l_1_nonlinear, W_2) + b_2\n",
    "y = tf.nn.softmax(l_1) # change to l_2 for MLP\n",
    "\n",
    "\n",
    "# y_ is a placeholder variable taking on the value of the target batch.\n",
    "y_ = tf.placeholder(tf.float32, [None, num_output])\n",
    "\n",
    "# computing cross entropy per sample\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])\n",
    "\n",
    "# averaging over samples\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Write the graph so we can look at it in TensorBoard\n",
    "# Now is a good time to try that\n",
    "# https://www.tensorflow.org/versions/r0.10/how_tos/summaries_and_tensorboard/index.html\n",
    "sw = tf.train.SummaryWriter('.', graph=tf.get_default_graph())\n",
    "\n",
    "# Create a placeholder we'll use later to feed the correct y value into the graph\n",
    "y_label = tf.placeholder(shape=[None], dtype=tf.float32, name='y_label')\n",
    "print(y_label)\n",
    "\n",
    "\n",
    "# Build training graph.\n",
    "# Create an operation that calculates loss.\n",
    "loss = tf.reduce_mean(tf.square(y - y_label))\n",
    "\n",
    "# Create an optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "# Create an operation that minimizes loss.\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "print(\"loss:\", loss)\n",
    "print(\"optimizer:\", optimizer)\n",
    "print(\"train:\", train)\n",
    "\n",
    "# Create an operation to initialize all the variables.\n",
    "init = tf.initialize_all_variables()\n",
    "print(init)\n",
    "\n",
    "# Create a session and launch the graph.\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "print(sess.run([W, b]))\n",
    "\n",
    "# Calculate accuracy on the evaluation data before training\n",
    "def eval():\n",
    "    return sess.run(loss, feed_dict={x: x_eval, y_label: y_eval})\n",
    "eval()\n",
    "\n",
    "# using the graph to print ops\n",
    "print(\"operations\")\n",
    "operations = [op.name for op in tf.get_default_graph().get_operations()]\n",
    "print(operations)\n",
    "print\n",
    "\n",
    "# variables are accessed through tensorflow\n",
    "print(\"variables\")\n",
    "variables = [var.name for var in tf.all_variables()]\n",
    "print(variables)\n",
    "\n",
    "\n",
    "\n",
    "# Buld inference graph.\n",
    "# Create Variables W and b that compute y_data = W * x_data + b\n",
    "W = tf.get_variable(shape=[], name='weights')\n",
    "b = tf.get_variable(shape=[], name='bias')\n",
    "\n",
    "# Uncomment the following lines to see W and b are.\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 文献\n",
    "\n",
    "### 1. \n",
    "title={Eigenfaces for Recognition},<br>\n",
    "author={Matthew Turk and Alex Pentland},<br>\n",
    "year={1991},<br>\n",
    "journal={Journal of Cognitive Neuroscience},<br>\n",
    "volume={3},<br>\n",
    "number={1},<br>\n",
    "pages={71-86},<br>\n",
    "abstract={We have developed a near-real-time computer system that can locate and track a subject's head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-0) recognition problem rather than requiring recovery of threedimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-0 characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as \"eigenfaces,\" because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easv to implement using a neural network 'architecture.},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "title={Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection},<br>\n",
    "author={Peter N. Belhumeur and Jo\\~{a}o P. Hespanha and David J. Kriegman},<br>\n",
    "year={1997},<br>\n",
    "journal={{IEEE} Transactions on Pattern Analysis and Machine Intelligence},<br>\n",
    "volume={19},<br>\n",
    "number={7},<br>\n",
    "pages={711-720},<br>\n",
    "abstract={We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying illumination but fixed pose, lie in a 3D linear subspace of the high dimensional image space—if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing, images will deviate from this linear subspace. Rather than explicitly modeling this deviation, we linearly project the image into a subspace in a manner which discounts those regions of the face with large deviation. Our projection method is based on Fisher’s Linear Discriminant and produces well separated classes in a low-dimensional subspace, even under severe variation in lighting and facial expressions. The Eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements. Yet, extensive experimental results demonstrate that the proposed “Fisherface” method has error rates that are lower than those of the Eigenface technique for tests on the Harvard and Yale Face Databases.}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
